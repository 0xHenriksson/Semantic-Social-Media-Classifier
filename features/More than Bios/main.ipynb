{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import mysql.connector\n",
    "import pinecone\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to OSU servers\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"classmysql.engr.oregonstate.edu\",\n",
    "  user=\"capstone_2023_tdsp1\",\n",
    "  password=\"Capstone1\",\n",
    "  database=\"capstone_2023_tdsp1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = mydb.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing bios and job titles\n",
    "cur.execute(\"select twitter_profiles.person_id,twitter_profiles.description, job_titles.title FROM twitter_profiles join people on twitter_profiles.person_id = people.id join positions on people.id = positions.person_id join job_titles on job_titles.id = positions.job_title_id where twitter_profiles.description is NOT NULL and job_titles.title is NOT NULL LIMIT 5000;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "myresult = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = np.array(myresult)\n",
    "test_list.reshape((len(test_list), len(test_list[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(myresult)):\n",
    "    myresult[i] = list(myresult[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(myresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={0:'person_id',1:'bio', 2:'job_title'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the urls in order to preprocess the text.\n",
    "def remove_urls(text):\n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'www\\S+', '', text)\n",
    "    text = re.sub(r'pic.twitter.com\\S+', '', text)\n",
    "    text = re.sub(r't\\.co\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "df[\"bio\"] = df[\"bio\"].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the emoji in order to preprocess the text.\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "df[\"bio\"] = df[\"bio\"].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# removing the stop words.\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "df[\"bio\"] = df[\"bio\"].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bio\"] = df[\"bio\"].apply(lambda x: lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bio\"] = df[\"bio\"].str.strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess the text because when we run the model on the text, the model gets all the important information to give better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_embedding = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the bio embeddings in order to send them to Pinecone.\n",
    "for i in range(df.shape[0]):\n",
    "    bio_embedding.append( (f'{i}',model.encode(df.iloc[i,1], convert_to_tensor=True).tolist(),{\"original_id\": str(df.iloc[i,0])}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title_embedding = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the job title embeddings in order to send them to Pinecone.\n",
    "for i in range(df.shape[0]):\n",
    "    job_title_embedding.append( (f'{i}',model.encode(df.iloc[i,2], convert_to_tensor=True).tolist(),{\"original_id\": str(df.iloc[i,0])}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(api_key=\"0d149ae1-ac52-4f2a-87af-d54693849369\", environment=\"us-east1-gcp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.Index(\"testbiojob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(iterable, batch_size=100):\n",
    "    \"\"\"A helper function to break an iterable into chunks of size batch_size.\"\"\"\n",
    "    it = iter(iterable)\n",
    "    chunk = tuple(itertools.islice(it, batch_size))\n",
    "    while chunk:\n",
    "        yield chunk\n",
    "        chunk = tuple(itertools.islice(it, batch_size))\n",
    "\n",
    "vector_dim = 768\n",
    "vector_count = 5000\n",
    "\n",
    "\n",
    "# Upsert data with 100 vectors per upsert request\n",
    "for ids_vectors_chunk in chunks(bio_embedding, batch_size=100):\n",
    "    index.upsert(vectors=ids_vectors_chunk,namespace='biov2')  # Assuming `index` defined elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(iterable, batch_size=100):\n",
    "    \"\"\"A helper function to break an iterable into chunks of size batch_size.\"\"\"\n",
    "    it = iter(iterable)\n",
    "    chunk = tuple(itertools.islice(it, batch_size))\n",
    "    while chunk:\n",
    "        yield chunk\n",
    "        chunk = tuple(itertools.islice(it, batch_size))\n",
    "\n",
    "vector_dim = 768\n",
    "vector_count = 5000\n",
    "\n",
    "\n",
    "# Upsert data with 100 vectors per upsert request\n",
    "for ids_vectors_chunk in chunks(job_title_embedding, batch_size=100):\n",
    "    index.upsert(vectors=ids_vectors_chunk,namespace='job_titlev2')  # Assuming `index` defined elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_bio = index.query(\n",
    "    vector=[bio_embedding[0][1]],\n",
    "    top_k = df.shape[0],\n",
    "    include_metadata=True,\n",
    "    namespace='biov2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_job_title = index.query(\n",
    "    vector=[job_title_embedding[0][1]],\n",
    "    top_k = df.shape[0],\n",
    "    include_metadata=True,\n",
    "    namespace='job_titlev2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_bio = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we just selected a random ID in order to get the cosine result for testing purposes.\n",
    "for i in top_bio['matches']:\n",
    "    if i['metadata']['original_id'] == '44534':\n",
    "        temp_bio = i['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_job = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we just selected a random ID in order to get the cosine result for testing purposes.\n",
    "for i in top_job_title['matches']:\n",
    "    if i['metadata']['original_id'] == '44534':\n",
    "        temp_job = i['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the weights, these could be a feature so that when a client wants to make the bio more important than the job title, then they could just set the weight for the bio bigger than the job title.\n",
    "bio_weight = 1\n",
    "job_weight = 1\n",
    "\n",
    "# calculate weighted average\n",
    "overall_similarity = (temp_bio * bio_weight + temp_job * job_weight) / (bio_weight + job_weight)\n",
    "print(\"Bio similarity score:\", temp_bio)\n",
    "print(\"Job title similarity score:\", temp_job)\n",
    "print(\"Overall similarity score:\", overall_similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
