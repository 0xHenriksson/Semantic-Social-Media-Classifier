{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "import pinecone\n",
    "import time\n",
    "import pandas\n",
    "from sqlalchemy import create_engine\n",
    "from enum import Enum\n",
    "import random\n",
    "import itertools\n",
    "from mysql import *\n",
    "import mysql.connector\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SystemLogging:\n",
    "    def __init__(self, loggingConfig):\n",
    "        self.name = None\n",
    "        self.system_start = None\n",
    "        self.system_stop = None\n",
    "        self.model_start = None\n",
    "        self.model_stop = None\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.input: list\n",
    "        self.output: list\n",
    "        self._input_len = None\n",
    "        self._output_len = None\n",
    "        self.device: str\n",
    "        self.pinecone_namespace = None\n",
    "        self.pinecone_index = None\n",
    "        self.model_dim = None\n",
    "        self.pinecone_Ustart = None\n",
    "        self.pinecone_Ustop = None\n",
    "        self.pinecone_Qstart = None\n",
    "        self.pinecone_Qstop = None\n",
    "        self.pinecone_Kmin = None\n",
    "        self.pinecone_Kmax = None\n",
    "        self.pinecone_Kavg = None\n",
    "        self._loader = loggingConfig\n",
    "        self._insert_stmnts = {\n",
    "            \"info\" : \"INSERT INTO exec_info (exec_start, exec_stop, input_size, output_size, name) VALUES (%s, %s, %s, %s, %s)\",\n",
    "            \"model\" : \"INSERT INTO exec_model (exec_id, tokenizer, model, mod_start, mod_stop, device, dim) VALUES ((SELECT exec_id FROM exec_info WHERE name = %s), %s, %s, %s, %s, %s, %s)\",\n",
    "            \"pinecone\" : \"INSERT INTO exec_pinecone (exec_id, namespace, exec_pinecone.index, upsert_start, upsert_stop, query_start, query_stop, kmin, kmax, kavg) VALUES ((SELECT exec_id FROM exec_info WHERE name = %s), %s, %s, %s, %s, %s, %s, %s, %s, %s)\",\n",
    "            \"input\" : \"INSERT INTO exec_input (exec_id, person_id) VALUES ((SELECT exec_id FROM exec_info WHERE name = %s), %s)\",\n",
    "            \"output\" : \"INSERT INTO exec_output (exec_id, person_id, k_value) VALUES ((SELECT exec_id FROM exec_info WHERE name = %s), %s, %s)\"\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "    \n",
    "    @input.setter\n",
    "    def input(self, value):\n",
    "        self._input_len = len(value) or None\n",
    "        self._input = value\n",
    "    \n",
    "    @property\n",
    "    def output(self):\n",
    "        return self._output\n",
    "    \n",
    "    @output.setter\n",
    "    def output(self, value):\n",
    "        self._output_len = len(value) or None\n",
    "        self._output = value\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._device\n",
    "    \n",
    "    @device.setter\n",
    "    def device(self, value):\n",
    "        if value == \"cuda:0\":\n",
    "            self._device = \"GPU\"\n",
    "        elif value == \"cpu\":\n",
    "            self._device = \"CPU\"\n",
    "        else:\n",
    "            raise ValueError(\"device must be of type cuda:0 or cpu\")\n",
    "        \n",
    "    def upload_to_db(self):\n",
    "        database = mysql.connector.connect(\n",
    "            host=self._loader.host,\n",
    "            user=self._loader.user,\n",
    "            password=self._loader.password,\n",
    "            database=self._loader.database\n",
    "        )\n",
    "        cursor = database.cursor()\n",
    "        info_data = (self.system_start, self.system_stop, self._input_len, self._output_len, self.name)\n",
    "        cursor.execute(self._insert_stmnts[\"info\"], info_data)\n",
    "        model_data = (self.name, self.model_start, self.model_stop, self.tokenizer, self.model, self.device, self.model_dim)\n",
    "        cursor.execute(self._insert_stmnts[\"model\"], model_data)\n",
    "        input_data = [(self.name, i) for i in self.input]\n",
    "        cursor.executemany(self._insert_stmnts[\"input\"], input_data)\n",
    "        output_data = [(self.name, i[0], round(i[1], 2)) for i in self.output]\n",
    "        cursor.executemany(self._insert_stmnts[\"output\"], output_data)\n",
    "        pinecone_data = (self.name, self.pinecone_namespace, self.pinecone_index, self.pinecone_Ustart, self.pinecone_Ustop, self.pinecone_Qstart, self.pinecone_Qstop, self.pinecone_Kmin, self.pinecone_Kmax, self.pinecone_Kavg)\n",
    "        cursor.execute(self._insert_stmnts[\"pinecone\"], pinecone_data)\n",
    "        database.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    def check_vars(self):\n",
    "        newlist = [self.name,\n",
    "        self.system_start,\n",
    "        self.system_stop,\n",
    "        self.model_start,\n",
    "        self.model_stop,\n",
    "        self.tokenizer,\n",
    "        self.model,\n",
    "        self.input,\n",
    "        self.output,\n",
    "        self._input_len,\n",
    "        self._output_len,\n",
    "        self.device,\n",
    "        self.pinecone_namespace,\n",
    "        self.pinecone_index,\n",
    "        self.model_dim,\n",
    "        self.pinecone_Ustart,\n",
    "        self.pinecone_Ustop,\n",
    "        self.pinecone_Qstart,\n",
    "        self.pinecone_Qstop,\n",
    "        self.pinecone_Kmin,\n",
    "        self.pinecone_Kmax,\n",
    "        self.pinecone_Kavg]\n",
    "        return newlist\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# If device was \"cuda:0\", then it means it's running on the GPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "databaseConfig:SimpleNamespace\n",
    "pineconeConfig:SimpleNamespace\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f, object_hook=lambda x: SimpleNamespace(**x))\n",
    "    databaseConfig = config.oregonstate.data\n",
    "    loggingConfig = config.oregonstate.logging\n",
    "    pineconeConfig = config.pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = input(\"Choose a name for this execution:\\n\")\n",
    "tokenizer_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "dimensions = 384\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "namespace = input(\"Specify a namespace to store the embeddings under:\\n\")\n",
    "limit = int(input(\"How many database entries would you like to embed? (Maximum)\\n\"))\n",
    "top_n = int(input(\"How many similar results would you like to see?\\n\"))\n",
    "logger = SystemLogging(loggingConfig)\n",
    "logger.system_start = start_time\n",
    "logger.tokenizer = tokenizer_name\n",
    "logger.model = model_name\n",
    "logger.model_dim = dimensions\n",
    "logger.input = [i for i in range(20)]\n",
    "logger.device = device\n",
    "logger.name = name\n",
    "logger.pinecone_index = pineconeConfig.index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Types:\n",
    "    class Connection(Enum):\n",
    "        FROM_TYPE_DB = 0\n",
    "        FROM_TYPE_CSV = 1\n",
    "        _READ_ID_LIST = 101\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eCairnConnector:\n",
    "    def __init__(self, method: Types.Connection, **kwargs):\n",
    "        self._kwargs = kwargs\n",
    "        self._data = None\n",
    "        if method == Types.Connection.FROM_TYPE_DB:\n",
    "            if \"db_config\" not in self._kwargs.keys():\n",
    "                raise ValueError(\"Expected to find value for `db_config`. No value found.\")\n",
    "            elif type(self._kwargs[\"db_config\"]) != SimpleNamespace:\n",
    "                raise TypeError(\"Expected to find type {} for `db_config`. Found type {}.\".format(SimpleNamespace, type(self._kwargs[\"db_config\"])))\n",
    "            else:\n",
    "                if \"limit\" in self._kwargs.keys():\n",
    "                    self._from_db(self._kwargs[\"db_config\"], limit=self._kwargs[\"limit\"])\n",
    "                else:\n",
    "                    self._from_db(self._kwargs[\"db_config\"])\n",
    "        elif method == Types.Connection.FROM_TYPE_CSV:\n",
    "            if \"csv_filename\" not in self._kwargs.keys():\n",
    "                raise ValueError(\"Expected to find value for `csv_filename`. No value found.\")\n",
    "            elif type(self._kwargs[\"csv_filename\"]) != str:\n",
    "                raise TypeError(\"Expected to find type {} for `csv_filename`. Found type {}.\".format(str, type(self._kwargs[\"csv_filename\"])))\n",
    "            else:\n",
    "                self._from_csv(self._kwargs[\"csv_filename\"])\n",
    "        elif method == Types.Connection._READ_ID_LIST:\n",
    "            if \"limit\" in self._kwargs.keys():\n",
    "                self._query_db_by_id(self._kwargs[\"id_list\"], limit = self._kwargs[\"limit\"])\n",
    "            else:\n",
    "                self._query_db_by_id(self._kwargs[\"id_list\"])\n",
    "\n",
    "\n",
    "    def _from_db(self, databaseConfig:SimpleNamespace, limit=1000) -> None:\n",
    "        uri = f'mariadb+mysqlconnector://{databaseConfig.user}:{databaseConfig.password}@{databaseConfig.host}/{databaseConfig.database}'\n",
    "        engine = create_engine(uri)\n",
    "        self._data = pandas.read_sql(f\"SELECT person_id, IFNULL(description,'') FROM twitter_profiles ORDER BY twitter_profiles.person_id ASC limit {limit};\", engine)\n",
    "\n",
    "    def _from_csv(self, file:str) -> None:\n",
    "        self._data = pandas.read_csv(file, index_col=0)\n",
    "\n",
    "    def _query_db_by_id(self, ref_list:list, limit = 10000) -> None:\n",
    "        uri = f'mariadb+mysqlconnector://{databaseConfig.user}:{databaseConfig.password}@{databaseConfig.host}/{databaseConfig.database}'\n",
    "        engine = create_engine(uri)\n",
    "        self._data = pandas.read_sql(f\"SELECT person_id, IFNULL(description,'') FROM twitter_profiles WHERE person_id IN ({','.join(map(lambda x: str(x), ref_list))}) ORDER BY twitter_profiles.person_id ASC limit {limit};\", engine)\n",
    "\n",
    "    def get_dataframe(self) -> pandas.DataFrame:\n",
    "        return self._data\n",
    "    \n",
    "    @classmethod\n",
    "    def get_eCairn_byID(cls, db_config:SimpleNamespace, ref_list:list) -> pandas.DataFrame:\n",
    "        new_instance = cls(Types.Connection._READ_ID_LIST, db_config=db_config, id_list = ref_list)\n",
    "        return new_instance.get_dataframe()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading up the model and the tokenizer\n",
    "def pretrained_setup(tokenizer_name:str, model_name:str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates the embeddings.\n",
    "def emb(text,model,tokenizer):\n",
    "    encoded_input = tokenizer(text =text, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    return np.array(sentence_embeddings.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if limit != 0:  \n",
    "    new_data = eCairnConnector(Types.Connection.FROM_TYPE_DB, db_config=databaseConfig, limit=limit)\n",
    "else:\n",
    "    new_data = eCairnConnector(Types.Connection.FROM_TYPE_DB, db_config=databaseConfig)\n",
    "test_list = new_data.get_dataframe()\n",
    "test_list = test_list.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495197\n"
     ]
    }
   ],
   "source": [
    "print(len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are creating for each bio an embedding + we are attaching an ID number. \n",
    "# An example: (\"id-0\",array(embeddings))\n",
    "tokenizer, model = pretrained_setup(tokenizer_name, model_name)\n",
    "logger.model_start = datetime.datetime.now()\n",
    "logger.pinecone_namespace = namespace\n",
    "start_time = time.time()\n",
    "next_read = time.time() + 5\n",
    "total = len(test_list)\n",
    "for item in test_list:\n",
    "    if time.time() > next_read:\n",
    "        print(f\"{len(embeddings_dataset)}/{total}\")\n",
    "        next_read = time.time() + 5\n",
    "    embeddings_dataset.append( (f'vector-{item[0]}',emb(str(item[1]),model,tokenizer)[0].tolist(), {\"original_id\": item[0]}) )\n",
    "logger.model_stop = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(api_key=pineconeConfig.key, environment=pineconeConfig.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the right index on pinecone\n",
    "index = pinecone.Index(pineconeConfig.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to upload the embeddings to pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def chunks(iterable, batch_size=100):\n",
    "    \"\"\"A helper function to break an iterable into chunks of size batch_size.\"\"\"\n",
    "    it = iter(iterable)\n",
    "    chunk = tuple(itertools.islice(it, batch_size))\n",
    "    while chunk:\n",
    "        yield chunk\n",
    "        chunk = tuple(itertools.islice(it, batch_size))\n",
    "\n",
    "vector_dim = 384\n",
    "vector_count = len(embeddings_dataset)\n",
    "\n",
    "\n",
    "# Upsert data with 100 vectors per upsert request\n",
    "logger.pinecone_Ustart = datetime.datetime.now()\n",
    "for ids_vectors_chunk in chunks(embeddings_dataset, batch_size=100):\n",
    "    index.upsert(vectors=ids_vectors_chunk, namespace=namespace)  # Assuming `index` defined elsewhere\n",
    "\n",
    "# with pinecone.Index('example-index', pool_threads=30) as index:\n",
    "#     # Send requests in parallel\n",
    "#     async_results = [\n",
    "#         index.upsert(vectors=ids_vectors_chunk, async_req=True)\n",
    "#         for ids_vectors_chunk in chunks(example_data_generator, batch_size=100)\n",
    "#     ]\n",
    "#     # Wait for and retrieve responses (this raises in case of error)\n",
    "#     [async_result.get() for async_result in async_results]\n",
    "logger.pinecone_Ustop = datetime.datetime.now()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are processing the output from the embedding search back into human readable information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.pinecone_Qstart = datetime.datetime.now()\n",
    "top = index.query(\n",
    "    vector=[embeddings_dataset[0][1]],\n",
    "    top_k = top_n+1,\n",
    "    include_values=False,\n",
    "    namespace=namespace,\n",
    "    include_metadata=True\n",
    ")\n",
    "logger.pinecone_Qstop = datetime.datetime.now()\n",
    "logger.output = [(int(i[\"metadata\"]['original_id']), float(i[\"score\"])) for i in top[\"matches\"][1:]]\n",
    "logger.pinecone_Kmin = min(map(lambda x: x[1], logger.output))\n",
    "logger.pinecone_Kmax = max(map(lambda x: x[1], logger.output))\n",
    "logger.pinecone_Kavg = float(np.average(list(map(lambda x: x[1], logger.output))))\n",
    "logger.system_stop = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.upload_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# maplist = list(map(lambda x : (int(x['id']), x['score']), top[\"matches\"]))\n",
    "# output = np.array(maplist,  dtype=(object))\n",
    "# oids, simvals = output.T\n",
    "# oids = oids.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mask = np.isin(element = test_list[:,0],test_elements = oids)\n",
    "# output_processed = test_list[mask]\n",
    "# readable = np.column_stack((output_processed, simvals))\n",
    "# with open(\"output.txt\", \"w\", encoding=\"UTF8\") as f:\n",
    "#     f.writelines(f'{sim}\\t| {line}\\n\\n\\n\\n' for vid, line, sim in readable)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a8d20506be82a4461f9b41041c14fdfe08e8897388d050f9d1ea4a80103d22a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
